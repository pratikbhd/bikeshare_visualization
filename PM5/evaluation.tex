\section{Evaluation} 
\label{sec:evaluation}

Our project can be put under the category of an application based visualization project. We used open-source data to create visualizations that help users identify trends and patterns in the way bike-share systems are being used in the city of Los Angeles. Keeping this in mind, we had planned our evaluation process to be primarily aimed at user test surveys.
The volunteers for user surveys can be one among the following:
\begin{enumerate}
	\item Mechanical Turks
	\item A group of people who can be considered as laymen in the field of data visualization
	\item A group of people with a considerable amount of idea and experience in the field of visualization design.
\end{enumerate}

Our evaluation approach included the last two groups of people from the list above. We decided not to move forward with performing evaluations on mechanical turks because we felt that for a project of our scale, being moderately complex and not too detailed, using the last two groups of people would suffice to come to a strong evaluation result. We explain each form of evaluation through the following sub-sections.

\subsection{Survey Users Q\&A}

This evaluation was based on asking our test users a number of questions with varying level of depth and accessing their answers in a qualitative and quantitative measure.

\subsubsection{Design}
As mentioned above, the user Q\&A session was primarily based on providing our test users with our visualization, making them study it in detail and then following it up with a number of questions. First off, we had to select the test users for our survey. Since this Q\&A session was aimed for laymen in the field of visualization, we selected friends who were not well acquainted with this subject matter as our test surveys. Our choice for only selecting laymen users was because the class presentation peer review would consist of review from a group of experienced data visualization users. To balance the selection, each project member initially selected two test subjects adding upto a total of six surveyors. Later, each member performed the evaluation on two more subjects each. This takes the total number of surveyors to 12.\newline
Then, we decided on method our survey. Each user was assigned to view and navigate through our visualization for 2 minutes. After this duration, the visualization was removed and they were handed a paper with a list of questionnaires. We wanted to see if our visualization design also had a sense of remembrance to it and by making the users answer questions by first removing the visualization we were able to test that to some extent. The questions were primarily of two kinds: open-ended and close-ended.\newline
The open-ended question pertained to the volunteers answering what they inferred from the overall visualization. This gave us a high-level feedback as to whether our choice of visualization fulfilled its initial objective. The close-ended questions will be more detailed and looked into whether the users were able to derive data and trends from the visualization. The rationale behind selecting these questions was to evaluate our design from both a high-level scenario and a detailed pattern finding scenario, which is ultimately the goal of the design. Some of the questions asked for each kind of questionnaire is listed below. A complete list of all the questions can be found in the \textbf{Evaluation} directory.
\begin{enumerate}
	\item Open-Ended Questions
	\begin{itemize}
		\item What can you infer from the visualization at a first glance?
		\item What do you think this visualization is trying to achieve?
		\item Does the visualization look appealing to you when you first see it?
	\end{itemize}
	\item Close-Ended Questions
	\begin{itemize}
		\item Which area of LA has more bike stations?
		\item How are the bike rents distributed across a certain area of bike stations?
		\item What overall trend can be inferred from the bike rent data?
	\end{itemize}
\end{enumerate}
We opted for comprehensive questions instead of multiple-answer questions which could ask subjects to answer from a list of 5 options ranging from 'Strongly Agree' to 'Strongly Disagree'. We decided on this because in addition to receiving positive/negative feedback, we also wanted feedback that would help us improve our design and add to it for future work. A simple multiple choice question would only provide us with a discrete result set for generalization.\newline
Each user was given 5 minutes to finish the questionnaire after which the answers were collected and analyzed. The answers from all test subjects were tallied and categorized broadly into two bins: positive and negative. The details from the answers were noted down which served well for our lessons learned and future work sections.
\subsubsection{Results}
The overall results from the user Q\&A session was positive. This section will talk about the summary of the observations and the responses received for the visualization design.\newline
Almost all the open-ended questions received positive responses. It could be concluded that the high-level goal was met by our visualization design. Out of the 12 participants, 10 answered positively on questions like "\textit{What do you think this visualization is trying to achieve?}". This gives us a success rate of 83.3\%. However, the reactions were a bit mixed towards the initial appeal of the visualization. Only 7 out of the 12 subjects positively responded towards the initial appeal and design. This makes up for 58.3\% of the subject population.\newline
Some comments about the design were directed towards the layout and look of the time histogram. We received feedback to make it more visually appealing and 6 of the subjects mentioned that the prevailing bug on our design was a a negative factor while the remaining half believed it did not act as a hindrance to their analysis.\newline
As for the close-ended questions, they were answered with relative ease. Tasks such as finding busiest bike stations and station distribution was done quickly and almost all the test subjects were able to determine the trend of bike rents. 11 out of the 12 subjects were successful in finding the trend of bike rents through time which is an accuracy of 91.6\%. We also tested the idea of remembrance to check how much of the visualization they still remember and received positive results as 9 out of 12 subjects agreed to remembering evey detail of the visualization (75\%).\newline
As mentioned before, all the answers of the questionnaire can be viewed in the \textbf{Evaluation} directory.
\subsection{Visualization Comparison}

This evaluation method is closely linked with the first method because it was performed at the same time one after the other. 
\subsubsection{Design}
In addition to finding out how our visualization design fared on its own (in terms of fulfilling its primary objectives), we also wanted to see how it compared to other visualizations that performed similar tasks as our design. Comparing two things which perform similar if not the same tasks is always a good way to check for effectiveness and efficiency. This test would expand our evaluation coverage in such a way that we could get more ideas and feedback about possible changes/addition to our design.\newline
Following from the first evaluation technique, we presented the test subjects with another visualization design. We chose the \textbf{Bike Visualization} website which contains the visualization of bike shares for a large number of locations. This design implements more interactions than our visualization and is much more complex than our system. So for the sake of equality in comparison, we decided to test this for the same tasks as that were performed on our visualization. We chose the \textbf{New York Citibike Visualization} where we asked our test users the same questions as before after letting them view and interact with the visualization for 2 minutes. In this case, we focused more on the close-ended questions since we wanted to compare trend finding and location finding tasks between the two visualizations.
\subsubsection{Results}
The results from the visualization comparison came out somewhat expected. The New York Citibike visualization had more features and interactions, making it more favored than our visualization. 11 out of the 12 test subjects leaned towards this visualization apporach (91.67\%).\newline
The open-ended questions provided similar results as well, both in terms of answer type and time to answer. These visualizations were built by a team of experts who had experience in the field of data visualization and hence the design choices made by them was sound and just. That makes the visualization look appealing and obtains the overall objective as well. All 12 out of 12 people liked the initial appeal of the visualization producing a success rate of 100\%. The same accuracy was obtained for questions like "\textit{What do you think this visualization is trying to achieve?}".\newline
Similarly, the close-ended questions did not vary as much from the answers of our visualization. The only noticeable difference was in the speed of answering. The test subjects seemed to take less time to find trends and locate items/places in the map compared to our visualization. This could be because of faster data processing and a smoother filtration process by the New York CitiBike visualization compared to our design.\newline
Overall, the visualization comparison resulted in test subjects favoring the New York CitiBike visualization over our visualization design.
\subsection{Class Peer Review}
This is the part of the evaluation where our visualization was reviewed by a group of students experienced in the field of data visualization.
\subsubsection{Design}
As part of our class project, we presented our visualization design to the class of CSC 544 - Advanced Data Visualization. There were 5 students and Professor Katherine Isaacs attending our presentation. During the presentation, we talked about the overview of our project, explained the methodology ranging from task and data abstraction to the design rationale. We explained our visualization design to the class along with a small demo. After the presentation, there was a round of Q\&A session where the students asked us a number of questions related to the project and also provided feedback. The students were then assigned to anonymously fill up a questionnaire designed by the Professor. The questionnaire was in the form of a Google Form and contained the following questions:
\begin{enumerate}
	\item What is the goal of this project? What problem is it trying to solve and why is that problem important?
	\item What are the strengths of the solution?
	\item What in the solution could be improved and how? (Do not repeat what the presenter has already said.)
\end{enumerate}
The answers from the questionnaire were then used to form our evaluation result.
\subsection{Results}
The answers to the questionnaires were analyzed and the following conclusions were drawn for each question:
\begin{enumerate}
	\item For the first question, all of the five reviewers accurately described the goal of the project including the problem its trying to solve and the importance of it. The responses matched our project goal to visualize the LA bike share system. For the importance of the problem, the responses varied from helping users see bike share trends to planning the distribution of bike shares from a commercial standpoint. All these responses do fall in the scope of importance of the problem.
	\item Our visualization received a number of positive reviews in terms of strengths. What was interesting was how varied the responses were between the five reviewers. This goes to show the advantage of using visualization experts for evaluation. Below are some of the strengths of the visualization as mentioned in the reviews:
	\begin{itemize}
		\item Good data source with large possibility for future design additions
		\item Smooth interactions in the visualization
		\item Trends were displayed in a pronounced and pre-attentive manner
		\item Low learning curve giving an intuitive sense to the visualization
		\item Detailed data and task abstraction
		\item Good and in-depth design rationale
		\item Detailed evaluation plan, especially the visualization comparison section
	\end{itemize}
	These reviews supported the positive feedback received from the previous two methods of evaluation.
	\item Along with the strengths, there were a number of areas of improvements that were mentioned. These points were noted on the reviews but were also discussed in the Q\&A session after the presentation. Some of the future areas of improvements are listed below:
	\begin{itemize}
		\item When zoomed out, the circles get large and end up overlapping and crowding each other. A better representation, maybe a voronoi diagram would be useful.
		\item The opacity of the circles are not low enough to not occlude the background so decreasing the opacity might be helpful in that.
		\item Adding a title to the visualization and labels to the graphs would be helpful.
		\item Adding a 'Play Button' animation that automatically cycles through each month of the time period to show the visualization would be a good design addition.
		\item Showing the destination when the source is clicked.
		\item Adding a button to reset the view of the map and center is back to the original location would be helpful in viewers to start over.
		\item Since this is a large project with good data source, using more attributes such as destination and trip route information to show more trends would add to the current progress.
		\item Showing the information of all the months at once using various color/color scales for each month might help in comparing the data between months.
		\item Being able to handle data with a longer time frame than the current two year frame.
	\end{itemize}
	The above ideas and recommendations for future improvements of the project matched with some of the future plans that we had as well.
\end{enumerate}
\subsection{Discussion}
Here, we talk about the results from our evaluations and how it shapes our current visualization and any future direction of it. In a nutshell, our visualization project was conceived in a very positive manner in terms of the problem it was trying to solve and the method implemented to solve it but it fell behind in terms of features and designs compared to other visualizations that perform the same task.\newline
Starting with the strengths, the results of the evaluation resulted showed that both the general audience as well as the experienced reviewers liked the implementation of the visualization design. The use of circles of varying radius to show different bike stations over the map of a city was a good design choice and the rationale behind the color/opacity of the marks was also received well. Most of the test subjects being able to answer both the open-ended high level questions and well as the close-ended trend finding questions added to the conclusion that the chosen design rationale suited well for the task performed. In addition to the design, our data/task abstraction and evaluation approach was also well received by the peer reviewers which in a resulted in our design choices being well-defined.\newline
Coming to the improvements, our visualization seemed to have large room for improvements and many possibilities for future work. This was evident from the visualization comparison and the peer reviews received from the presentation. The project idea we took on and the problem we tried to solve through it is of a large domain. Through the background research, we were aware of the variety of tasks and interactions that could be performed to solve this problem by using the raw data available to us. Our primary goal for this project was to design a visualization layout to solve the problem of finding the bike share trends and how it changed with respect to time. There are a number of areas where our design can be improved to better portray the information. Some of the more urgent improvements seem to be adding titles and labels to inform users about what is being visualized and modifying the way in which the data is mapped when zoomed out so that the circles don't crowd over each other.\newline
In addition to this, we have improvements of our own which were part of our milestone but could not be achieved in time. These include fixing some bugs related to data mapping and implementing a brush over the time histogram in order to select multiple months at a time. The results from the visualization comparison show that our data handling and processing could be improved to speed up the visualization and interactions. One way of doing this would be to load the data to a server and read through it instead of reading the data locally. This would also provide options for scalability which is another area of improvement. The New York CitiBike visualization shows us that there a number of attributes that can be mapped into the visualization to show more trends and make the analysis process more richer. Other improvements include making the interaction smoother and more user-friendly by adding automatic animations and map view reset buttons. These are great ideas that can be added to our visualization to improve user experience.\newline
In summary, the results from the evaluations show that our visualization project performs its intended task with high accuracy but at the same time needs improvements on its designs along with integration of other attributes to expand the data and trend analysis of the bike share system.
\subsection{Limitations}
While our evaluation approach produced acceptable results that concurred with the results from the peer review, there were a few limitations to our approach. First, the questions asked to the test subjects were of a kind that did not produce detailed answers. The questions were more focused upon whether someone understood the problem and the task being performed in the visualization and if a user could easily perform said task. While these questions are important in order to properly evaluate the effectiveness of the visualization design, questions similar to the ones asked in the peer review section would have helped us gain insights about what could be improved from a layman's perspective. Even though design suggestions from a visualization expert will always be more detailed, taking suggestions from these test subjects would have been beneficial. This is because they form a part of the demographic population who despite mostly being laymen in the field of visualization will be the largest proportion of consumers of this visualization. Even if their suggestions might be in the simple scope of improvements, they will directly be targeted towards improving the user experience. So, questions that ask the users about possible improvements and suggestions on the design will help in better directing the future efforts of the project.\newline
Another limitation was the ineffective implementation of quantitative evaluation. During our initial evaluation phase, we intended to time the users while they were answering the trend finding and data localization questions. Our aim was to compare the time taken by each user and get a general sense of the time it takes for them to solve these tasks. Then we had planned on timing them for the same tasks on the New York CitiBike visualization and comparing these two sets of data. Our method of having the test subjects first go through the visualization and then answer the questiona separately posed a problem in terms of timing them. This is because the amount of time spent by the users on answering the questions now also relied on how well they remembered the visualization itself. We wanted the time to be independent of this factor and only depend on how effective the visualization itself is. Though the first few evaluations were timed, we later discarded this approach and relied on a general sense of how long people took to answer the questions.\newline
One way of adding quantitative evaluation to our appoach would be to having the users solve some task or find some data while they are navigating the visualization. This way the timing data obtained would be a factor of the effectiveness of the visualization design alone. We think this would result in more accurate data and hence a better evaluation result.